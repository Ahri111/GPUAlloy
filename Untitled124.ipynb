{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TePyNVFEbQkz",
        "outputId": "ef11dea4-83f2-4159-e1b5-21de7df31f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "JAX Devices: [CudaDevice(id=0)]\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "CORRECTNESS CHECK\n",
            "============================================================\n",
            "✓ swap_by_idx: all versions match\n",
            "✓ sample_sublattice_swap: all versions match\n",
            "✓ beam_search: all versions match\n",
            "\n",
            "✅ All correctness checks passed!\n",
            "\n",
            "============================================================\n",
            "BENCHMARK SETUP\n",
            "============================================================\n",
            "Batch size: 10000\n",
            "N swaps: 10000\n",
            "Atoms per structure: 160\n",
            "B-site (Ti↔Fe): 32 positions\n",
            "O-site (O↔VO): 96 positions\n",
            "Beam size: 4\n",
            "\n",
            "Generated 10000 structures: (10000, 160)\n",
            "\n",
            "============================================================\n",
            "TEST 1: swap_by_idx (단일 스왑)\n",
            "============================================================\n",
            "Original (2x .at[].set())               :       0.11 ms ± 0.02\n",
            "Optimized (single scatter)              :       0.10 ms ± 0.03\n",
            "Optimized v2 (permutation)              :       0.11 ms ± 0.01\n",
            "\n",
            "============================================================\n",
            "TEST 2: sample_sublattice_swap (단일 sublattice 스왑)\n",
            "============================================================\n",
            "Original                                :       0.16 ms ± 0.02\n",
            "Optimized                               :       0.17 ms ± 0.01\n",
            "\n",
            "============================================================\n",
            "TEST 3: apply_n_swaps (10000번 연속 스왑) ⭐\n",
            "============================================================\n",
            "Original                                :     509.10 ms ± 0.92\n",
            "Optimized                               :     469.10 ms ± 1.07\n",
            "\n",
            "============================================================\n",
            "TEST 4: beam_search (beam_size=4)\n",
            "============================================================\n",
            "Original (nested vmap)                  :       1.02 ms ± 0.12\n",
            "Optimized (flatten→swap→reshape)        :       0.87 ms ± 0.08\n",
            "\n",
            "============================================================\n",
            "SPEEDUP SUMMARY\n",
            "============================================================\n",
            "swap_by_idx                   : 1.03x speedup\n",
            "swap_by_idx (v2)              : 0.93x speedup\n",
            "sublattice_swap               : 0.93x speedup\n",
            "apply_n_swaps (10000x)        : 1.09x speedup\n",
            "beam_search                   : 1.16x speedup\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Full Benchmark: Original vs Optimized Swap Operations\n",
        "Sr(Ti,Fe)(O,VO)3 Perovskite System - Functional Style (No Classes)\n",
        "\n",
        "비교 항목:\n",
        "1. swap_by_idx: 단일 스왑\n",
        "2. sample_sublattice_swap: 단일 sublattice 스왑\n",
        "3. apply_n_swaps: N번 연속 스왑 (scan)\n",
        "4. sample_sublattice_swap_beam: 빔서치\n",
        "\n",
        "Benchmark:\n",
        "- batch_size = 10000\n",
        "- n_swaps = 10000\n",
        "\"\"\"\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, lax\n",
        "from functools import partial\n",
        "from typing import Tuple, Optional, NamedTuple\n",
        "import time\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"JAX Devices:\", jax.devices())\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Data Structures\n",
        "# =============================================================================\n",
        "\n",
        "class SwapResult(NamedTuple):\n",
        "    swapped: jnp.ndarray\n",
        "    indices: jnp.ndarray\n",
        "\n",
        "class BeamResult(NamedTuple):\n",
        "    swapped: jnp.ndarray\n",
        "    indices: jnp.ndarray\n",
        "    log_probs: jnp.ndarray\n",
        "\n",
        "ATOM_TYPES = {\"Sr\": 0, \"Ti\": 1, \"Fe\": 2, \"O\": 3, \"VO\": 4}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Structure Generator\n",
        "# =============================================================================\n",
        "\n",
        "@partial(jax.jit, static_argnums=(1, 2, 3, 4, 5, 6))\n",
        "def generate_structures(\n",
        "    key: random.PRNGKey,\n",
        "    batch_size: int,\n",
        "    n_Sr: int, n_Ti: int, n_Fe: int, n_O: int, n_VO: int,\n",
        ") -> jnp.ndarray:\n",
        "    \"\"\"Generate random perovskite structures.\"\"\"\n",
        "    n_B = n_Ti + n_Fe\n",
        "    n_Osite = n_O + n_VO\n",
        "\n",
        "    key_b, key_o = random.split(key)\n",
        "\n",
        "    b_template = jnp.concatenate([\n",
        "        jnp.ones(n_Ti, dtype=jnp.int32) * ATOM_TYPES[\"Ti\"],\n",
        "        jnp.ones(n_Fe, dtype=jnp.int32) * ATOM_TYPES[\"Fe\"],\n",
        "    ])\n",
        "    o_template = jnp.concatenate([\n",
        "        jnp.ones(n_O, dtype=jnp.int32) * ATOM_TYPES[\"O\"],\n",
        "        jnp.ones(n_VO, dtype=jnp.int32) * ATOM_TYPES[\"VO\"],\n",
        "    ])\n",
        "\n",
        "    # Random permutation via argsort\n",
        "    noise_b = random.uniform(key_b, (batch_size, n_B))\n",
        "    noise_o = random.uniform(key_o, (batch_size, n_Osite))\n",
        "    b_configs = b_template[jnp.argsort(noise_b, axis=-1)]\n",
        "    o_configs = o_template[jnp.argsort(noise_o, axis=-1)]\n",
        "\n",
        "    a_configs = jnp.ones((batch_size, n_Sr), dtype=jnp.int32) * ATOM_TYPES[\"Sr\"]\n",
        "\n",
        "    return jnp.concatenate([a_configs, b_configs, o_configs], axis=-1)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ORIGINAL: 네 코드 그대로\n",
        "# =============================================================================\n",
        "\n",
        "@jax.jit\n",
        "def orig_swap_by_idx(x: jnp.ndarray, idx: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Original: .at[].set() 2회\"\"\"\n",
        "    batch_size = x.shape[0]\n",
        "    batch_idx = jnp.arange(batch_size)\n",
        "    idx_a, idx_b = idx[:, 0], idx[:, 1]\n",
        "    val_a = x[batch_idx, idx_a]\n",
        "    val_b = x[batch_idx, idx_b]\n",
        "    x_swapped = x.at[batch_idx, idx_a].set(val_b)\n",
        "    x_swapped = x_swapped.at[batch_idx, idx_b].set(val_a)\n",
        "    return x_swapped\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2, 3))\n",
        "def orig_sample_sublattice_swap(\n",
        "    key: random.PRNGKey,\n",
        "    atom_types: jnp.ndarray,\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    scores: Optional[jnp.ndarray] = None,\n",
        ") -> SwapResult:\n",
        "    \"\"\"Original single sublattice swap\"\"\"\n",
        "    sub_idx = jnp.array(sublattice_indices)\n",
        "    batch_size, N = atom_types.shape\n",
        "    M = len(sub_idx)\n",
        "\n",
        "    sub_types = atom_types[:, sub_idx]\n",
        "    is_a = (sub_types == type_a)\n",
        "    is_b = (sub_types == type_b)\n",
        "\n",
        "    sub_scores = jnp.zeros((batch_size, M)) if scores is None else scores[:, sub_idx]\n",
        "\n",
        "    key_a, key_b = random.split(key)\n",
        "    gumbel_a = random.gumbel(key_a, (batch_size, M))\n",
        "    gumbel_b = random.gumbel(key_b, (batch_size, M))\n",
        "\n",
        "    score_a = jnp.where(is_a, sub_scores + gumbel_a, -jnp.inf)\n",
        "    score_b = jnp.where(is_b, sub_scores + gumbel_b, -jnp.inf)\n",
        "\n",
        "    local_a = jnp.argmax(score_a, axis=-1)\n",
        "    local_b = jnp.argmax(score_b, axis=-1)\n",
        "\n",
        "    global_a = sub_idx[local_a]\n",
        "    global_b = sub_idx[local_b]\n",
        "    indices = jnp.stack([global_a, global_b], axis=-1)\n",
        "\n",
        "    swapped = orig_swap_by_idx(atom_types, indices)\n",
        "    return SwapResult(swapped=swapped, indices=indices)\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(3, 4, 5, 6))\n",
        "def orig_apply_n_swaps(\n",
        "    key: random.PRNGKey,\n",
        "    atom_types: jnp.ndarray,\n",
        "    scores: Optional[jnp.ndarray],\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    n_swaps: int,\n",
        ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"Original N-step swap with lax.scan\"\"\"\n",
        "    keys = random.split(key, n_swaps)\n",
        "\n",
        "    def scan_fn(carry, key_i):\n",
        "        x = carry\n",
        "        result = orig_sample_sublattice_swap(\n",
        "            key_i, x, sublattice_indices, type_a, type_b, scores\n",
        "        )\n",
        "        return result.swapped, result.indices\n",
        "\n",
        "    final, all_indices = lax.scan(scan_fn, atom_types, keys)\n",
        "    return final, all_indices\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2, 3, 5))\n",
        "def orig_beam_search(\n",
        "    atom_types: jnp.ndarray,\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    scores: jnp.ndarray,\n",
        "    beam_size: int = 4,\n",
        ") -> BeamResult:\n",
        "    \"\"\"Original beam search with nested vmap\"\"\"\n",
        "    sub_idx = jnp.array(sublattice_indices)\n",
        "    batch_size, N = atom_types.shape\n",
        "    M = len(sub_idx)\n",
        "\n",
        "    sub_types = atom_types[:, sub_idx]\n",
        "    sub_scores = scores[:, sub_idx]\n",
        "\n",
        "    is_a = (sub_types == type_a)\n",
        "    is_b = (sub_types == type_b)\n",
        "\n",
        "    score_a = jnp.where(is_a, sub_scores, -jnp.inf)\n",
        "    score_b = jnp.where(is_b, sub_scores, -jnp.inf)\n",
        "\n",
        "    top_scores_a, top_local_a = lax.top_k(score_a, beam_size)\n",
        "    top_scores_b, top_local_b = lax.top_k(score_b, beam_size)\n",
        "\n",
        "    pair_scores = top_scores_a[:, :, None] + top_scores_b[:, None, :]\n",
        "    pair_scores_flat = pair_scores.reshape(batch_size, -1)\n",
        "    top_pair_scores, top_pair_idx = lax.top_k(pair_scores_flat, beam_size)\n",
        "\n",
        "    idx_a_beam = top_pair_idx // beam_size\n",
        "    idx_b_beam = top_pair_idx % beam_size\n",
        "\n",
        "    batch_idx = jnp.arange(batch_size)[:, None]\n",
        "    local_a_beam = top_local_a[batch_idx, idx_a_beam]\n",
        "    local_b_beam = top_local_b[batch_idx, idx_b_beam]\n",
        "\n",
        "    global_a_beam = sub_idx[local_a_beam]\n",
        "    global_b_beam = sub_idx[local_b_beam]\n",
        "    indices_candidates = jnp.stack([global_a_beam, global_b_beam], axis=-1)\n",
        "\n",
        "    # Nested vmap (네가 만든 방식)\n",
        "    def swap_single_beam(x, idx):\n",
        "        return orig_swap_by_idx(x[None, :], idx[None, :])[0]\n",
        "    def swap_all_beams(x, indices):\n",
        "        return jax.vmap(lambda idx: swap_single_beam(x, idx))(indices)\n",
        "    swapped_candidates = jax.vmap(swap_all_beams)(atom_types, indices_candidates)\n",
        "\n",
        "    log_prob_a = jax.nn.log_softmax(score_a, axis=-1)\n",
        "    log_prob_b = jax.nn.log_softmax(score_b, axis=-1)\n",
        "    lp_a = log_prob_a[batch_idx, local_a_beam]\n",
        "    lp_b = log_prob_b[batch_idx, local_b_beam]\n",
        "    log_probs = lp_a + lp_b\n",
        "\n",
        "    return BeamResult(swapped=swapped_candidates, indices=indices_candidates, log_probs=log_probs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# OPTIMIZED: 내가 제안한 최적화\n",
        "# =============================================================================\n",
        "\n",
        "@jax.jit\n",
        "def opt_swap_by_idx(x: jnp.ndarray, idx: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Optimized: single scatter\"\"\"\n",
        "    batch_size, N = x.shape\n",
        "    batch_idx = jnp.arange(batch_size)\n",
        "    idx_a, idx_b = idx[:, 0], idx[:, 1]\n",
        "    val_a = x[batch_idx, idx_a]\n",
        "    val_b = x[batch_idx, idx_b]\n",
        "\n",
        "    # Single scatter\n",
        "    scatter_indices = jnp.stack([\n",
        "        jnp.stack([batch_idx, idx_a], axis=1),\n",
        "        jnp.stack([batch_idx, idx_b], axis=1)\n",
        "    ], axis=0).reshape(-1, 2)\n",
        "    scatter_values = jnp.concatenate([val_b, val_a])\n",
        "\n",
        "    x_flat = x.reshape(-1)\n",
        "    flat_indices = scatter_indices[:, 0] * N + scatter_indices[:, 1]\n",
        "    x_swapped = x_flat.at[flat_indices].set(scatter_values)\n",
        "    return x_swapped.reshape(batch_size, N)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def opt_swap_by_idx_v2(x: jnp.ndarray, idx: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Optimized v2: permutation (TPU-friendly)\"\"\"\n",
        "    batch_size, N = x.shape\n",
        "    batch_idx = jnp.arange(batch_size)\n",
        "    idx_a, idx_b = idx[:, 0], idx[:, 1]\n",
        "\n",
        "    perm = jnp.broadcast_to(jnp.arange(N), (batch_size, N))\n",
        "    perm = perm.at[batch_idx, idx_a].set(idx_b)\n",
        "    perm = perm.at[batch_idx, idx_b].set(idx_a)\n",
        "    return jnp.take_along_axis(x, perm, axis=1)\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2, 3))\n",
        "def opt_sample_sublattice_swap(\n",
        "    key: random.PRNGKey,\n",
        "    atom_types: jnp.ndarray,\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    scores: Optional[jnp.ndarray] = None,\n",
        ") -> SwapResult:\n",
        "    \"\"\"Optimized single swap using opt_swap_by_idx\"\"\"\n",
        "    sub_idx = jnp.array(sublattice_indices)\n",
        "    batch_size, N = atom_types.shape\n",
        "    M = len(sub_idx)\n",
        "\n",
        "    sub_types = atom_types[:, sub_idx]\n",
        "    is_a = (sub_types == type_a)\n",
        "    is_b = (sub_types == type_b)\n",
        "\n",
        "    sub_scores = jnp.zeros((batch_size, M)) if scores is None else scores[:, sub_idx]\n",
        "\n",
        "    key_a, key_b = random.split(key)\n",
        "    gumbel_a = random.gumbel(key_a, (batch_size, M))\n",
        "    gumbel_b = random.gumbel(key_b, (batch_size, M))\n",
        "\n",
        "    score_a = jnp.where(is_a, sub_scores + gumbel_a, -jnp.inf)\n",
        "    score_b = jnp.where(is_b, sub_scores + gumbel_b, -jnp.inf)\n",
        "\n",
        "    local_a = jnp.argmax(score_a, axis=-1)\n",
        "    local_b = jnp.argmax(score_b, axis=-1)\n",
        "\n",
        "    indices = jnp.stack([sub_idx[local_a], sub_idx[local_b]], axis=-1)\n",
        "    swapped = opt_swap_by_idx(atom_types, indices)\n",
        "    return SwapResult(swapped=swapped, indices=indices)\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(3, 4, 5, 6))\n",
        "def opt_apply_n_swaps(\n",
        "    key: random.PRNGKey,\n",
        "    atom_types: jnp.ndarray,\n",
        "    scores: Optional[jnp.ndarray],\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    n_swaps: int,\n",
        ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"Optimized N-step swap\"\"\"\n",
        "    keys = random.split(key, n_swaps)\n",
        "\n",
        "    def scan_fn(carry, key_i):\n",
        "        x = carry\n",
        "        result = opt_sample_sublattice_swap(\n",
        "            key_i, x, sublattice_indices, type_a, type_b, scores\n",
        "        )\n",
        "        return result.swapped, result.indices\n",
        "\n",
        "    final, all_indices = lax.scan(scan_fn, atom_types, keys)\n",
        "    return final, all_indices\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2, 3, 5))\n",
        "def opt_beam_search(\n",
        "    atom_types: jnp.ndarray,\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    scores: jnp.ndarray,\n",
        "    beam_size: int = 4,\n",
        ") -> BeamResult:\n",
        "    \"\"\"Optimized beam search: flatten → single swap → reshape\"\"\"\n",
        "    sub_idx = jnp.array(sublattice_indices)\n",
        "    batch_size, N = atom_types.shape\n",
        "    M = len(sub_idx)\n",
        "\n",
        "    sub_types = atom_types[:, sub_idx]\n",
        "    sub_scores = scores[:, sub_idx]\n",
        "\n",
        "    is_a = (sub_types == type_a)\n",
        "    is_b = (sub_types == type_b)\n",
        "\n",
        "    score_a = jnp.where(is_a, sub_scores, -jnp.inf)\n",
        "    score_b = jnp.where(is_b, sub_scores, -jnp.inf)\n",
        "\n",
        "    top_scores_a, top_local_a = lax.top_k(score_a, beam_size)\n",
        "    top_scores_b, top_local_b = lax.top_k(score_b, beam_size)\n",
        "\n",
        "    pair_scores = top_scores_a[:, :, None] + top_scores_b[:, None, :]\n",
        "    pair_scores_flat = pair_scores.reshape(batch_size, -1)\n",
        "    top_pair_scores, top_pair_idx = lax.top_k(pair_scores_flat, beam_size)\n",
        "\n",
        "    idx_a_beam = top_pair_idx // beam_size\n",
        "    idx_b_beam = top_pair_idx % beam_size\n",
        "\n",
        "    batch_idx = jnp.arange(batch_size)[:, None]\n",
        "    local_a_beam = top_local_a[batch_idx, idx_a_beam]\n",
        "    local_b_beam = top_local_b[batch_idx, idx_b_beam]\n",
        "\n",
        "    global_a_beam = sub_idx[local_a_beam]\n",
        "    global_b_beam = sub_idx[local_b_beam]\n",
        "    indices_candidates = jnp.stack([global_a_beam, global_b_beam], axis=-1)\n",
        "\n",
        "    # KEY OPTIMIZATION: flatten batch*beam → single swap call → reshape\n",
        "    expanded_types = jnp.broadcast_to(\n",
        "        atom_types[:, None, :], (batch_size, beam_size, N)\n",
        "    ).reshape(batch_size * beam_size, N)\n",
        "\n",
        "    flat_indices = indices_candidates.reshape(batch_size * beam_size, 2)\n",
        "    swapped_flat = opt_swap_by_idx(expanded_types, flat_indices)\n",
        "    swapped_candidates = swapped_flat.reshape(batch_size, beam_size, N)\n",
        "\n",
        "    log_prob_a = jax.nn.log_softmax(score_a, axis=-1)\n",
        "    log_prob_b = jax.nn.log_softmax(score_b, axis=-1)\n",
        "    lp_a = log_prob_a[batch_idx, local_a_beam]\n",
        "    lp_b = log_prob_b[batch_idx, local_b_beam]\n",
        "    log_probs = lp_a + lp_b\n",
        "\n",
        "    return BeamResult(swapped=swapped_candidates, indices=indices_candidates, log_probs=log_probs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# BENCHMARK\n",
        "# =============================================================================\n",
        "\n",
        "def benchmark(name, fn, *args, n_warmup=3, n_trials=10):\n",
        "    \"\"\"Run benchmark with warmup\"\"\"\n",
        "\n",
        "    def wait_ready(out):\n",
        "        \"\"\"Handle different return types\"\"\"\n",
        "        if hasattr(out, 'swapped'):  # SwapResult or BeamResult\n",
        "            out.swapped.block_until_ready()\n",
        "        elif isinstance(out, tuple):\n",
        "            out[0].block_until_ready()\n",
        "        elif hasattr(out, 'block_until_ready'):  # raw array\n",
        "            out.block_until_ready()\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(n_warmup):\n",
        "        out = fn(*args) if args else fn()\n",
        "        wait_ready(out)\n",
        "\n",
        "    # Benchmark\n",
        "    times = []\n",
        "    for _ in range(n_trials):\n",
        "        start = time.perf_counter()\n",
        "        out = fn(*args) if args else fn()\n",
        "        wait_ready(out)\n",
        "        times.append(time.perf_counter() - start)\n",
        "\n",
        "    avg = sum(times) / len(times)\n",
        "    std = (sum((t - avg)**2 for t in times) / len(times)) ** 0.5\n",
        "    print(f\"{name:40s}: {avg*1000:10.2f} ms ± {std*1000:.2f}\")\n",
        "    return avg\n",
        "\n",
        "\n",
        "def run_all_benchmarks():\n",
        "    \"\"\"Run full benchmark suite\"\"\"\n",
        "\n",
        "    # Setup\n",
        "    BATCH_SIZE = 10000\n",
        "    N_SWAPS = 10000\n",
        "    BEAM_SIZE = 4\n",
        "\n",
        "    composition = {\"Sr\": 32, \"Ti\": 8, \"Fe\": 24, \"O\": 84, \"VO\": 12}\n",
        "    N = sum(composition.values())\n",
        "\n",
        "    n_B = composition[\"Ti\"] + composition[\"Fe\"]\n",
        "    n_O_total = composition[\"O\"] + composition[\"VO\"]\n",
        "\n",
        "    # Python int로 명시적 변환 (JAX Array가 들어가면 hash 에러)\n",
        "    sr_end = int(composition[\"Sr\"])\n",
        "    b_end = int(composition[\"Sr\"] + n_B)\n",
        "    b_site_idx = tuple(range(sr_end, b_end))\n",
        "    o_site_idx = tuple(range(b_end, int(N)))\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BENCHMARK SETUP\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Batch size: {BATCH_SIZE}\")\n",
        "    print(f\"N swaps: {N_SWAPS}\")\n",
        "    print(f\"Atoms per structure: {N}\")\n",
        "    print(f\"B-site (Ti↔Fe): {len(b_site_idx)} positions\")\n",
        "    print(f\"O-site (O↔VO): {len(o_site_idx)} positions\")\n",
        "    print(f\"Beam size: {BEAM_SIZE}\")\n",
        "\n",
        "    # Generate structures\n",
        "    key = random.PRNGKey(42)\n",
        "    key, subkey = random.split(key)\n",
        "\n",
        "    structures = generate_structures(\n",
        "        subkey, BATCH_SIZE,\n",
        "        composition[\"Sr\"], composition[\"Ti\"], composition[\"Fe\"],\n",
        "        composition[\"O\"], composition[\"VO\"]\n",
        "    )\n",
        "    structures.block_until_ready()\n",
        "    print(f\"\\nGenerated {BATCH_SIZE} structures: {structures.shape}\")\n",
        "\n",
        "    # Random scores\n",
        "    key, subkey = random.split(key)\n",
        "    scores = random.normal(subkey, structures.shape)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # =========================================================================\n",
        "    # Test 1: Single swap_by_idx\n",
        "    # =========================================================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TEST 1: swap_by_idx (단일 스왑)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    idx = random.randint(subkey, (BATCH_SIZE, 2), 0, N)\n",
        "\n",
        "    results[\"orig_swap\"] = benchmark(\"Original (2x .at[].set())\", orig_swap_by_idx, structures, idx)\n",
        "    results[\"opt_swap\"] = benchmark(\"Optimized (single scatter)\", opt_swap_by_idx, structures, idx)\n",
        "    results[\"opt_swap_v2\"] = benchmark(\"Optimized v2 (permutation)\", opt_swap_by_idx_v2, structures, idx)\n",
        "\n",
        "    # =========================================================================\n",
        "    # Test 2: Single sublattice swap\n",
        "    # =========================================================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TEST 2: sample_sublattice_swap (단일 sublattice 스왑)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    results[\"orig_sublattice\"] = benchmark(\n",
        "        \"Original\",\n",
        "        lambda: orig_sample_sublattice_swap(subkey, structures, b_site_idx, 1, 2, scores)\n",
        "    )\n",
        "    results[\"opt_sublattice\"] = benchmark(\n",
        "        \"Optimized\",\n",
        "        lambda: opt_sample_sublattice_swap(subkey, structures, b_site_idx, 1, 2, scores)\n",
        "    )\n",
        "\n",
        "    # =========================================================================\n",
        "    # Test 3: N swaps (핵심 테스트)\n",
        "    # =========================================================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TEST 3: apply_n_swaps ({N_SWAPS}번 연속 스왑) ⭐\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    results[\"orig_n_swaps\"] = benchmark(\n",
        "        \"Original\",\n",
        "        orig_apply_n_swaps, subkey, structures, scores, b_site_idx, 1, 2, N_SWAPS,\n",
        "        n_warmup=2, n_trials=5\n",
        "    )\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    results[\"opt_n_swaps\"] = benchmark(\n",
        "        \"Optimized\",\n",
        "        opt_apply_n_swaps, subkey, structures, scores, b_site_idx, 1, 2, N_SWAPS,\n",
        "        n_warmup=2, n_trials=5\n",
        "    )\n",
        "\n",
        "    # =========================================================================\n",
        "    # Test 4: Beam search\n",
        "    # =========================================================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TEST 4: beam_search (beam_size={BEAM_SIZE})\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    results[\"orig_beam\"] = benchmark(\n",
        "        \"Original (nested vmap)\",\n",
        "        orig_beam_search, structures, b_site_idx, 1, 2, scores, BEAM_SIZE\n",
        "    )\n",
        "    results[\"opt_beam\"] = benchmark(\n",
        "        \"Optimized (flatten→swap→reshape)\",\n",
        "        opt_beam_search, structures, b_site_idx, 1, 2, scores, BEAM_SIZE\n",
        "    )\n",
        "\n",
        "    # =========================================================================\n",
        "    # Summary\n",
        "    # =========================================================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"SPEEDUP SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    comparisons = [\n",
        "        (\"swap_by_idx\", \"orig_swap\", \"opt_swap\"),\n",
        "        (\"swap_by_idx (v2)\", \"orig_swap\", \"opt_swap_v2\"),\n",
        "        (\"sublattice_swap\", \"orig_sublattice\", \"opt_sublattice\"),\n",
        "        (f\"apply_n_swaps ({N_SWAPS}x)\", \"orig_n_swaps\", \"opt_n_swaps\"),\n",
        "        (\"beam_search\", \"orig_beam\", \"opt_beam\"),\n",
        "    ]\n",
        "\n",
        "    for name, orig_key, opt_key in comparisons:\n",
        "        speedup = results[orig_key] / results[opt_key]\n",
        "        print(f\"{name:30s}: {speedup:.2f}x speedup\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def verify_correctness():\n",
        "    \"\"\"Verify optimized versions match original\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CORRECTNESS CHECK\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    key = random.PRNGKey(123)\n",
        "    batch_size = 100\n",
        "\n",
        "    composition = {\"Sr\": 32, \"Ti\": 8, \"Fe\": 24, \"O\": 84, \"VO\": 12}\n",
        "    N = sum(composition.values())\n",
        "    n_B = composition[\"Ti\"] + composition[\"Fe\"]\n",
        "    # Python int로 명시적 변환\n",
        "    sr_end = int(composition[\"Sr\"])\n",
        "    b_end = int(sr_end + n_B)\n",
        "    b_site_idx = tuple(range(sr_end, b_end))\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    structures = generate_structures(\n",
        "        subkey, batch_size,\n",
        "        composition[\"Sr\"], composition[\"Ti\"], composition[\"Fe\"],\n",
        "        composition[\"O\"], composition[\"VO\"]\n",
        "    )\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    scores = random.normal(subkey, structures.shape)\n",
        "\n",
        "    # Test swap_by_idx\n",
        "    key, subkey = random.split(key)\n",
        "    idx = random.randint(subkey, (batch_size, 2), 0, N)\n",
        "\n",
        "    out_orig = orig_swap_by_idx(structures, idx)\n",
        "    out_opt = opt_swap_by_idx(structures, idx)\n",
        "    out_opt_v2 = opt_swap_by_idx_v2(structures, idx)\n",
        "\n",
        "    assert jnp.allclose(out_orig, out_opt), \"swap_by_idx mismatch!\"\n",
        "    assert jnp.allclose(out_orig, out_opt_v2), \"swap_by_idx_v2 mismatch!\"\n",
        "    print(\"✓ swap_by_idx: all versions match\")\n",
        "\n",
        "    # Test sublattice swap\n",
        "    test_key = random.PRNGKey(999)\n",
        "\n",
        "    res_orig = orig_sample_sublattice_swap(test_key, structures, b_site_idx, 1, 2, scores)\n",
        "    res_opt = opt_sample_sublattice_swap(test_key, structures, b_site_idx, 1, 2, scores)\n",
        "\n",
        "    assert jnp.allclose(res_orig.indices, res_opt.indices), \"sublattice indices mismatch!\"\n",
        "    assert jnp.allclose(res_orig.swapped, res_opt.swapped), \"sublattice swapped mismatch!\"\n",
        "    print(\"✓ sample_sublattice_swap: all versions match\")\n",
        "\n",
        "    # Test beam search\n",
        "    beam_orig = orig_beam_search(structures, b_site_idx, 1, 2, scores, 4)\n",
        "    beam_opt = opt_beam_search(structures, b_site_idx, 1, 2, scores, 4)\n",
        "\n",
        "    assert jnp.allclose(beam_orig.indices, beam_opt.indices), \"beam indices mismatch!\"\n",
        "    assert jnp.allclose(beam_orig.swapped, beam_opt.swapped), \"beam swapped mismatch!\"\n",
        "    assert jnp.allclose(beam_orig.log_probs, beam_opt.log_probs), \"beam log_probs mismatch!\"\n",
        "    print(\"✓ beam_search: all versions match\")\n",
        "\n",
        "    print(\"\\n✅ All correctness checks passed!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    verify_correctness()\n",
        "    run_all_benchmarks()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Sublattice-Constrained Swap Operations (JAX)\n",
        "Sr(Ti,Fe)(O,VO)3 Perovskite System\n",
        "\n",
        "Original Implementation - Proven Fastest\n",
        "\"\"\"\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random, lax\n",
        "from functools import partial\n",
        "from typing import Tuple, Optional, NamedTuple\n",
        "import time\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"JAX Devices:\", jax.devices())\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Data Structures\n",
        "# =============================================================================\n",
        "\n",
        "class SwapResult(NamedTuple):\n",
        "    swapped: jnp.ndarray\n",
        "    indices: jnp.ndarray\n",
        "\n",
        "class BeamResult(NamedTuple):\n",
        "    swapped: jnp.ndarray\n",
        "    indices: jnp.ndarray\n",
        "    log_probs: jnp.ndarray\n",
        "\n",
        "ATOM_TYPES = {\"Sr\": 0, \"Ti\": 1, \"Fe\": 2, \"O\": 3, \"VO\": 4}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Structure Generator\n",
        "# =============================================================================\n",
        "\n",
        "@partial(jax.jit, static_argnums=(1, 2, 3, 4, 5, 6))\n",
        "def generate_structures(\n",
        "    key: random.PRNGKey,\n",
        "    batch_size: int,\n",
        "    n_Sr: int, n_Ti: int, n_Fe: int, n_O: int, n_VO: int,\n",
        ") -> jnp.ndarray:\n",
        "    \"\"\"Generate random perovskite structures.\"\"\"\n",
        "    n_B = n_Ti + n_Fe\n",
        "    n_Osite = n_O + n_VO\n",
        "\n",
        "    key_b, key_o = random.split(key)\n",
        "\n",
        "    b_template = jnp.concatenate([\n",
        "        jnp.ones(n_Ti, dtype=jnp.int32) * ATOM_TYPES[\"Ti\"],\n",
        "        jnp.ones(n_Fe, dtype=jnp.int32) * ATOM_TYPES[\"Fe\"],\n",
        "    ])\n",
        "    o_template = jnp.concatenate([\n",
        "        jnp.ones(n_O, dtype=jnp.int32) * ATOM_TYPES[\"O\"],\n",
        "        jnp.ones(n_VO, dtype=jnp.int32) * ATOM_TYPES[\"VO\"],\n",
        "    ])\n",
        "\n",
        "    noise_b = random.uniform(key_b, (batch_size, n_B))\n",
        "    noise_o = random.uniform(key_o, (batch_size, n_Osite))\n",
        "    b_configs = b_template[jnp.argsort(noise_b, axis=-1)]\n",
        "    o_configs = o_template[jnp.argsort(noise_o, axis=-1)]\n",
        "\n",
        "    a_configs = jnp.ones((batch_size, n_Sr), dtype=jnp.int32) * ATOM_TYPES[\"Sr\"]\n",
        "\n",
        "    return jnp.concatenate([a_configs, b_configs, o_configs], axis=-1)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Core Swap Operations\n",
        "# =============================================================================\n",
        "\n",
        "@jax.jit\n",
        "def swap_by_idx(x: jnp.ndarray, idx: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"\n",
        "    Swap elements at specified indices.\n",
        "    Args:\n",
        "        x: [batch, N] tensor\n",
        "        idx: [batch, 2] indices to swap\n",
        "    Returns:\n",
        "        x_swapped: [batch, N]\n",
        "    \"\"\"\n",
        "    batch_size = x.shape[0]\n",
        "    batch_idx = jnp.arange(batch_size)\n",
        "    idx_a, idx_b = idx[:, 0], idx[:, 1]\n",
        "    val_a = x[batch_idx, idx_a]\n",
        "    val_b = x[batch_idx, idx_b]\n",
        "    x_swapped = x.at[batch_idx, idx_a].set(val_b)\n",
        "    x_swapped = x_swapped.at[batch_idx, idx_b].set(val_a)\n",
        "    return x_swapped\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2, 3))\n",
        "def sample_sublattice_swap(\n",
        "    key: random.PRNGKey,\n",
        "    atom_types: jnp.ndarray,\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    scores: Optional[jnp.ndarray] = None,\n",
        ") -> SwapResult:\n",
        "    \"\"\"\n",
        "    Sample one constrained swap per batch element.\n",
        "    Args:\n",
        "        key: JAX random key\n",
        "        atom_types: [batch, N] atom type indices\n",
        "        sublattice_indices: tuple of indices in sublattice (static for JIT)\n",
        "        type_a, type_b: types to swap\n",
        "        scores: [batch, N] swap scores (None = uniform)\n",
        "    Returns:\n",
        "        SwapResult with swapped tensor and indices\n",
        "    \"\"\"\n",
        "    sub_idx = jnp.array(sublattice_indices)\n",
        "    batch_size, N = atom_types.shape\n",
        "    M = len(sub_idx)\n",
        "\n",
        "    sub_types = atom_types[:, sub_idx]\n",
        "    is_a = (sub_types == type_a)\n",
        "    is_b = (sub_types == type_b)\n",
        "\n",
        "    sub_scores = jnp.zeros((batch_size, M)) if scores is None else scores[:, sub_idx]\n",
        "\n",
        "    key_a, key_b = random.split(key)\n",
        "    gumbel_a = random.gumbel(key_a, (batch_size, M))\n",
        "    gumbel_b = random.gumbel(key_b, (batch_size, M))\n",
        "\n",
        "    score_a = jnp.where(is_a, sub_scores + gumbel_a, -jnp.inf)\n",
        "    score_b = jnp.where(is_b, sub_scores + gumbel_b, -jnp.inf)\n",
        "\n",
        "    local_a = jnp.argmax(score_a, axis=-1)\n",
        "    local_b = jnp.argmax(score_b, axis=-1)\n",
        "\n",
        "    global_a = sub_idx[local_a]\n",
        "    global_b = sub_idx[local_b]\n",
        "    indices = jnp.stack([global_a, global_b], axis=-1)\n",
        "\n",
        "    swapped = swap_by_idx(atom_types, indices)\n",
        "    return SwapResult(swapped=swapped, indices=indices)\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2, 3))\n",
        "def sample_sublattice_swap_deterministic(\n",
        "    atom_types: jnp.ndarray,\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    scores: jnp.ndarray,\n",
        ") -> SwapResult:\n",
        "    \"\"\"Deterministic swap (no noise, pick highest scores).\"\"\"\n",
        "    sub_idx = jnp.array(sublattice_indices)\n",
        "    batch_size = atom_types.shape[0]\n",
        "    M = len(sub_idx)\n",
        "\n",
        "    sub_types = atom_types[:, sub_idx]\n",
        "    sub_scores = scores[:, sub_idx]\n",
        "\n",
        "    is_a = (sub_types == type_a)\n",
        "    is_b = (sub_types == type_b)\n",
        "\n",
        "    score_a = jnp.where(is_a, sub_scores, -jnp.inf)\n",
        "    score_b = jnp.where(is_b, sub_scores, -jnp.inf)\n",
        "\n",
        "    local_a = jnp.argmax(score_a, axis=-1)\n",
        "    local_b = jnp.argmax(score_b, axis=-1)\n",
        "\n",
        "    global_a = sub_idx[local_a]\n",
        "    global_b = sub_idx[local_b]\n",
        "    indices = jnp.stack([global_a, global_b], axis=-1)\n",
        "\n",
        "    swapped = swap_by_idx(atom_types, indices)\n",
        "    return SwapResult(swapped=swapped, indices=indices)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Multiple Swap Steps\n",
        "# =============================================================================\n",
        "\n",
        "@partial(jax.jit, static_argnums=(3, 4, 5, 6))\n",
        "def apply_n_swaps(\n",
        "    key: random.PRNGKey,\n",
        "    atom_types: jnp.ndarray,\n",
        "    scores: Optional[jnp.ndarray],\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    n_swaps: int,\n",
        ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"\n",
        "    Apply n swap steps using lax.scan (single sublattice).\n",
        "    Args:\n",
        "        key: random key\n",
        "        atom_types: [batch, N]\n",
        "        scores: [batch, N] or None\n",
        "        sublattice_indices: tuple\n",
        "        type_a, type_b: types\n",
        "        n_swaps: number of steps\n",
        "    Returns:\n",
        "        final: [batch, N]\n",
        "        all_indices: [n_swaps, batch, 2]\n",
        "    \"\"\"\n",
        "    keys = random.split(key, n_swaps)\n",
        "\n",
        "    def scan_fn(carry, key_i):\n",
        "        x = carry\n",
        "        result = sample_sublattice_swap(\n",
        "            key_i, x, sublattice_indices, type_a, type_b, scores\n",
        "        )\n",
        "        return result.swapped, result.indices\n",
        "\n",
        "    final, all_indices = lax.scan(scan_fn, atom_types, keys)\n",
        "    return final, all_indices\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(3, 4, 5, 6, 7, 8, 9))\n",
        "def apply_n_swaps_both(\n",
        "    key: random.PRNGKey,\n",
        "    atom_types: jnp.ndarray,\n",
        "    scores: Optional[jnp.ndarray],\n",
        "    b_site_indices: Tuple[int, ...],\n",
        "    o_site_indices: Tuple[int, ...],\n",
        "    type_ti: int,\n",
        "    type_fe: int,\n",
        "    type_o: int,\n",
        "    type_vo: int,\n",
        "    n_swaps: int,\n",
        ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"\n",
        "    Apply n swap steps with random B/O site selection (like PyTorch 'both' mode).\n",
        "    Args:\n",
        "        key: random key\n",
        "        atom_types: [batch, N]\n",
        "        scores: [batch, N] or None\n",
        "        b_site_indices: B-site tuple\n",
        "        o_site_indices: O-site tuple\n",
        "        type_ti, type_fe: B-site types\n",
        "        type_o, type_vo: O-site types\n",
        "        n_swaps: number of steps\n",
        "    Returns:\n",
        "        final: [batch, N]\n",
        "        all_indices: [n_swaps, batch, 2]\n",
        "    \"\"\"\n",
        "    keys = random.split(key, n_swaps * 2).reshape(n_swaps, 2, 2)\n",
        "\n",
        "    b_idx = jnp.array(b_site_indices)\n",
        "    o_idx = jnp.array(o_site_indices)\n",
        "\n",
        "    def scan_fn(carry, keys_i):\n",
        "        x = carry\n",
        "        key_choice, key_swap = keys_i[0], keys_i[1]\n",
        "\n",
        "        # Random choice: B-site or O-site (50/50)\n",
        "        do_b = random.uniform(key_choice) < 0.5\n",
        "\n",
        "        # Compute both swaps\n",
        "        result_b = sample_sublattice_swap(\n",
        "            key_swap, x, b_site_indices, type_ti, type_fe, scores\n",
        "        )\n",
        "        result_o = sample_sublattice_swap(\n",
        "            key_swap, x, o_site_indices, type_o, type_vo, scores\n",
        "        )\n",
        "\n",
        "        # Select based on random choice\n",
        "        swapped = jnp.where(do_b, result_b.swapped, result_o.swapped)\n",
        "        indices = jnp.where(do_b, result_b.indices, result_o.indices)\n",
        "\n",
        "        return swapped, indices\n",
        "\n",
        "    final, all_indices = lax.scan(scan_fn, atom_types, keys)\n",
        "    return final, all_indices\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Beam Search\n",
        "# =============================================================================\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2, 3, 5))\n",
        "def sample_sublattice_swap_beam(\n",
        "    atom_types: jnp.ndarray,\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    scores: jnp.ndarray,\n",
        "    beam_size: int = 4,\n",
        ") -> BeamResult:\n",
        "    \"\"\"\n",
        "    Beam search for top-k constrained swap candidates.\n",
        "    Args:\n",
        "        atom_types: [batch, N]\n",
        "        sublattice_indices: tuple of sublattice indices\n",
        "        type_a, type_b: types to swap\n",
        "        scores: [batch, N] swap scores\n",
        "        beam_size: number of candidates\n",
        "    Returns:\n",
        "        BeamResult with candidates, indices, and log_probs\n",
        "    \"\"\"\n",
        "    sub_idx = jnp.array(sublattice_indices)\n",
        "    batch_size, N = atom_types.shape\n",
        "    M = len(sub_idx)\n",
        "\n",
        "    sub_types = atom_types[:, sub_idx]\n",
        "    sub_scores = scores[:, sub_idx]\n",
        "\n",
        "    is_a = (sub_types == type_a)\n",
        "    is_b = (sub_types == type_b)\n",
        "\n",
        "    score_a = jnp.where(is_a, sub_scores, -jnp.inf)\n",
        "    score_b = jnp.where(is_b, sub_scores, -jnp.inf)\n",
        "\n",
        "    top_scores_a, top_local_a = lax.top_k(score_a, beam_size)\n",
        "    top_scores_b, top_local_b = lax.top_k(score_b, beam_size)\n",
        "\n",
        "    pair_scores = top_scores_a[:, :, None] + top_scores_b[:, None, :]\n",
        "    pair_scores_flat = pair_scores.reshape(batch_size, -1)\n",
        "    top_pair_scores, top_pair_idx = lax.top_k(pair_scores_flat, beam_size)\n",
        "\n",
        "    idx_a_beam = top_pair_idx // beam_size\n",
        "    idx_b_beam = top_pair_idx % beam_size\n",
        "\n",
        "    batch_idx = jnp.arange(batch_size)[:, None]\n",
        "    local_a_beam = top_local_a[batch_idx, idx_a_beam]\n",
        "    local_b_beam = top_local_b[batch_idx, idx_b_beam]\n",
        "\n",
        "    global_a_beam = sub_idx[local_a_beam]\n",
        "    global_b_beam = sub_idx[local_b_beam]\n",
        "    indices_candidates = jnp.stack([global_a_beam, global_b_beam], axis=-1)\n",
        "\n",
        "    def swap_single_beam(x, idx):\n",
        "        return swap_by_idx(x[None, :], idx[None, :])[0]\n",
        "    def swap_all_beams(x, indices):\n",
        "        return jax.vmap(lambda idx: swap_single_beam(x, idx))(indices)\n",
        "    swapped_candidates = jax.vmap(swap_all_beams)(atom_types, indices_candidates)\n",
        "\n",
        "    log_prob_a = jax.nn.log_softmax(score_a, axis=-1)\n",
        "    log_prob_b = jax.nn.log_softmax(score_b, axis=-1)\n",
        "    lp_a = log_prob_a[batch_idx, local_a_beam]\n",
        "    lp_b = log_prob_b[batch_idx, local_b_beam]\n",
        "    log_probs = lp_a + lp_b\n",
        "\n",
        "    return BeamResult(swapped=swapped_candidates, indices=indices_candidates, log_probs=log_probs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Log Probability\n",
        "# =============================================================================\n",
        "\n",
        "@partial(jax.jit, static_argnums=(1, 2, 3))\n",
        "def log_prob_sublattice_swap(\n",
        "    scores: jnp.ndarray,\n",
        "    sublattice_indices: Tuple[int, ...],\n",
        "    type_a: int,\n",
        "    type_b: int,\n",
        "    atom_types: jnp.ndarray,\n",
        "    swap_indices: jnp.ndarray,\n",
        ") -> jnp.ndarray:\n",
        "    \"\"\"\n",
        "    Compute log P(swap_indices | scores).\n",
        "    Args:\n",
        "        scores: [batch, N]\n",
        "        sublattice_indices: tuple\n",
        "        type_a, type_b: swapped types\n",
        "        atom_types: [batch, N] BEFORE swap\n",
        "        swap_indices: [batch, 2]\n",
        "    Returns:\n",
        "        log_probs: [batch]\n",
        "    \"\"\"\n",
        "    sub_idx = jnp.array(sublattice_indices)\n",
        "    batch_size = scores.shape[0]\n",
        "\n",
        "    sub_types = atom_types[:, sub_idx]\n",
        "    sub_scores = scores[:, sub_idx]\n",
        "\n",
        "    is_a = (sub_types == type_a)\n",
        "    is_b = (sub_types == type_b)\n",
        "\n",
        "    score_a = jnp.where(is_a, sub_scores, -jnp.inf)\n",
        "    score_b = jnp.where(is_b, sub_scores, -jnp.inf)\n",
        "\n",
        "    log_prob_a = jax.nn.log_softmax(score_a, axis=-1)\n",
        "    log_prob_b = jax.nn.log_softmax(score_b, axis=-1)\n",
        "\n",
        "    global_a = swap_indices[:, 0]\n",
        "    global_b = swap_indices[:, 1]\n",
        "\n",
        "    local_a = jnp.argmax(sub_idx[None, :] == global_a[:, None], axis=-1)\n",
        "    local_b = jnp.argmax(sub_idx[None, :] == global_b[:, None], axis=-1)\n",
        "\n",
        "    batch_idx = jnp.arange(batch_size)\n",
        "    lp_a = log_prob_a[batch_idx, local_a]\n",
        "    lp_b = log_prob_b[batch_idx, local_b]\n",
        "\n",
        "    return lp_a + lp_b\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Utility\n",
        "# =============================================================================\n",
        "\n",
        "def get_sublattice_indices(\n",
        "    atom_types: jnp.ndarray,\n",
        "    target_types: Tuple[int, ...],\n",
        ") -> Tuple[int, ...]:\n",
        "    \"\"\"Get indices where atom_types is in target_types.\"\"\"\n",
        "    mask = jnp.zeros(atom_types.shape, dtype=bool)\n",
        "    for t in target_types:\n",
        "        mask = mask | (atom_types == t)\n",
        "    indices = jnp.where(mask)[0]\n",
        "    return tuple(int(i) for i in indices)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Benchmark\n",
        "# =============================================================================\n",
        "\n",
        "def benchmark(name, fn, *args, n_warmup=3, n_trials=10):\n",
        "    \"\"\"Run benchmark with warmup\"\"\"\n",
        "\n",
        "    def wait_ready(out):\n",
        "        if hasattr(out, 'swapped'):\n",
        "            out.swapped.block_until_ready()\n",
        "        elif isinstance(out, tuple):\n",
        "            out[0].block_until_ready()\n",
        "        elif hasattr(out, 'block_until_ready'):\n",
        "            out.block_until_ready()\n",
        "\n",
        "    for _ in range(n_warmup):\n",
        "        out = fn(*args) if args else fn()\n",
        "        wait_ready(out)\n",
        "\n",
        "    times = []\n",
        "    for _ in range(n_trials):\n",
        "        start = time.perf_counter()\n",
        "        out = fn(*args) if args else fn()\n",
        "        wait_ready(out)\n",
        "        times.append(time.perf_counter() - start)\n",
        "\n",
        "    avg = sum(times) / len(times)\n",
        "    std = (sum((t - avg)**2 for t in times) / len(times)) ** 0.5\n",
        "    print(f\"{name:40s}: {avg*1000:10.2f} ms ± {std*1000:.2f}\")\n",
        "    return avg\n",
        "\n",
        "\n",
        "def run_benchmark():\n",
        "    \"\"\"Run benchmark suite\"\"\"\n",
        "\n",
        "    BATCH_SIZE = 10000\n",
        "    N_SWAPS = 10000\n",
        "    BEAM_SIZE = 4\n",
        "\n",
        "    composition = {\"Sr\": 32, \"Ti\": 8, \"Fe\": 24, \"O\": 84, \"VO\": 12}\n",
        "    N = sum(composition.values())\n",
        "    n_B = composition[\"Ti\"] + composition[\"Fe\"]\n",
        "    n_O = composition[\"O\"] + composition[\"VO\"]\n",
        "\n",
        "    sr_end = int(composition[\"Sr\"])\n",
        "    b_end = int(sr_end + n_B)\n",
        "    b_site_idx = tuple(range(sr_end, b_end))\n",
        "    o_site_idx = tuple(range(b_end, int(N)))\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BENCHMARK: batch={BATCH_SIZE}, n_swaps={N_SWAPS}, N={N}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"B-site: {len(b_site_idx)} positions (Ti={composition['Ti']}, Fe={composition['Fe']})\")\n",
        "    print(f\"O-site: {len(o_site_idx)} positions (O={composition['O']}, VO={composition['VO']})\")\n",
        "\n",
        "    key = random.PRNGKey(42)\n",
        "    key, subkey = random.split(key)\n",
        "\n",
        "    structures = generate_structures(\n",
        "        subkey, BATCH_SIZE,\n",
        "        composition[\"Sr\"], composition[\"Ti\"], composition[\"Fe\"],\n",
        "        composition[\"O\"], composition[\"VO\"]\n",
        "    )\n",
        "    structures.block_until_ready()\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    scores = random.normal(subkey, structures.shape)\n",
        "\n",
        "    # Single swap\n",
        "    print(f\"\\n[1] swap_by_idx\")\n",
        "    key, subkey = random.split(key)\n",
        "    idx = random.randint(subkey, (BATCH_SIZE, 2), 0, N)\n",
        "    benchmark(\"swap_by_idx\", swap_by_idx, structures, idx)\n",
        "\n",
        "    # Sublattice swap\n",
        "    print(f\"\\n[2] sample_sublattice_swap\")\n",
        "    key, subkey = random.split(key)\n",
        "    benchmark(\"B-site swap\",\n",
        "              lambda: sample_sublattice_swap(subkey, structures, b_site_idx, 1, 2, scores))\n",
        "    key, subkey = random.split(key)\n",
        "    benchmark(\"O-site swap\",\n",
        "              lambda: sample_sublattice_swap(subkey, structures, o_site_idx, 3, 4, scores))\n",
        "\n",
        "    # N swaps - B-site only\n",
        "    print(f\"\\n[3] apply_n_swaps B-site only ({N_SWAPS}x)\")\n",
        "    key, subkey = random.split(key)\n",
        "    benchmark(\"B-site only\",\n",
        "              apply_n_swaps, subkey, structures, scores, b_site_idx, 1, 2, N_SWAPS,\n",
        "              n_warmup=2, n_trials=5)\n",
        "\n",
        "    # N swaps - BOTH mode (공정한 비교!)\n",
        "    print(f\"\\n[4] apply_n_swaps BOTH mode ({N_SWAPS}x) ⭐ PyTorch 비교용\")\n",
        "    key, subkey = random.split(key)\n",
        "    benchmark(\"BOTH (B+O random)\",\n",
        "              apply_n_swaps_both, subkey, structures, scores,\n",
        "              b_site_idx, o_site_idx, 1, 2, 3, 4, N_SWAPS,\n",
        "              n_warmup=2, n_trials=5)\n",
        "\n",
        "    # Beam search\n",
        "    print(f\"\\n[5] beam_search (k={BEAM_SIZE})\")\n",
        "    benchmark(\"B-site beam\",\n",
        "              sample_sublattice_swap_beam, structures, b_site_idx, 1, 2, scores, BEAM_SIZE)\n",
        "    benchmark(\"O-site beam\",\n",
        "              sample_sublattice_swap_beam, structures, o_site_idx, 3, 4, scores, BEAM_SIZE)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Example\n",
        "# =============================================================================\n",
        "\n",
        "def example():\n",
        "    \"\"\"Example usage\"\"\"\n",
        "    key = random.PRNGKey(42)\n",
        "\n",
        "    composition = {\"Sr\": 32, \"Ti\": 8, \"Fe\": 24, \"O\": 84, \"VO\": 12}\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    structures = generate_structures(\n",
        "        subkey, 4,\n",
        "        composition[\"Sr\"], composition[\"Ti\"], composition[\"Fe\"],\n",
        "        composition[\"O\"], composition[\"VO\"]\n",
        "    )\n",
        "\n",
        "    b_site_idx = tuple(range(32, 64))\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    scores = random.normal(subkey, structures.shape)\n",
        "\n",
        "    print(\"=== Example ===\")\n",
        "    print(f\"Structures shape: {structures.shape}\")\n",
        "\n",
        "    # Single swap\n",
        "    key, subkey = random.split(key)\n",
        "    result = sample_sublattice_swap(subkey, structures, b_site_idx, 1, 2, scores)\n",
        "    print(f\"Swap indices: {result.indices}\")\n",
        "\n",
        "    # Beam search\n",
        "    beam = sample_sublattice_swap_beam(structures, b_site_idx, 1, 2, scores, 4)\n",
        "    print(f\"Beam candidates shape: {beam.swapped.shape}\")\n",
        "    print(f\"Beam log_probs: {beam.log_probs[0]}\")\n",
        "\n",
        "    # N swaps\n",
        "    key, subkey = random.split(key)\n",
        "    final, all_idx = apply_n_swaps(subkey, structures, scores, b_site_idx, 1, 2, 10)\n",
        "    print(f\"After 10 swaps: {final.shape}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example()\n",
        "    run_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmh_1cyL-P3c",
        "outputId": "4247f812-337d-4fd9-a3c3-aeb22667ffe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "JAX Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n",
            "============================================================\n",
            "=== Example ===\n",
            "Structures shape: (4, 160)\n",
            "Swap indices: [[60 57]\n",
            " [59 63]\n",
            " [42 46]\n",
            " [61 63]]\n",
            "Beam candidates shape: (4, 4, 160)\n",
            "Beam log_probs: [-2.664374  -2.982929  -3.3346233 -3.4151826]\n",
            "After 10 swaps: (4, 160)\n",
            "\n",
            "============================================================\n",
            "BENCHMARK: batch=10000, n_swaps=10000, N=160\n",
            "============================================================\n",
            "B-site: 32 positions (Ti=8, Fe=24)\n",
            "O-site: 96 positions (O=84, VO=12)\n",
            "\n",
            "[1] swap_by_idx\n",
            "swap_by_idx                             :       0.58 ms ± 0.02\n",
            "\n",
            "[2] sample_sublattice_swap\n",
            "B-site swap                             :       0.67 ms ± 0.01\n",
            "O-site swap                             :       0.86 ms ± 0.01\n",
            "\n",
            "[3] apply_n_swaps B-site only (10000x)\n",
            "B-site only                             :    3614.92 ms ± 0.41\n",
            "\n",
            "[4] apply_n_swaps BOTH mode (10000x) ⭐ PyTorch 비교용\n",
            "BOTH (B+O random)                       :    8956.15 ms ± 0.26\n",
            "\n",
            "[5] beam_search (k=4)\n",
            "B-site beam                             :       3.89 ms ± 0.08\n",
            "O-site beam                             :       6.43 ms ± 0.25\n"
          ]
        }
      ]
    }
  ]
}