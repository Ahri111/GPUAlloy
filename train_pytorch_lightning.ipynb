{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGFormer Training with PyTorch Lightning\n",
    "\n",
    "Cleaned up training code for:\n",
    "1. **Energy Function Learning** - Crystal property prediction (MAE loss)\n",
    "2. **Swap-based Structure Search** - REINFORCE training\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup\n",
    "import os\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/CGformer\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q torch torchvision torchaudio\n",
    "    !pip install -q pytorch-lightning\n",
    "    !pip install -q torch-geometric\n",
    "    !pip install -q pymatgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Lightning: {pl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from model import CrystalGraphConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import CIFData, collate_pool, get_train_val_test_loader\n",
    "\n",
    "\n",
    "class CrystalDataModule(pl.LightningDataModule):\n",
    "    \"\"\"PyTorch Lightning DataModule for crystal data.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        batch_size: int = 16,\n",
    "        train_ratio: float = 0.7,\n",
    "        val_ratio: float = 0.15,\n",
    "        test_ratio: float = 0.15,\n",
    "        num_workers: int = 0,\n",
    "        max_num_nbr: int = 12,\n",
    "        radius: float = 8.0,\n",
    "        random_seed: int = 123,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.num_workers = num_workers\n",
    "        self.max_num_nbr = max_num_nbr\n",
    "        self.radius = radius\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        self.dataset = None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if self.dataset is None:\n",
    "            self.dataset = CIFData(\n",
    "                self.root_dir,\n",
    "                max_num_nbr=self.max_num_nbr,\n",
    "                radius=self.radius,\n",
    "                random_seed=self.random_seed\n",
    "            )\n",
    "            \n",
    "            self.train_loader, self.val_loader, self.test_loader = get_train_val_test_loader(\n",
    "                dataset=self.dataset,\n",
    "                collate_fn=collate_pool,\n",
    "                batch_size=self.batch_size,\n",
    "                train_ratio=self.train_ratio,\n",
    "                val_ratio=self.val_ratio,\n",
    "                test_ratio=self.test_ratio,\n",
    "                return_test=True,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=torch.cuda.is_available(),\n",
    "                train_size=None,\n",
    "                val_size=None,\n",
    "                test_size=None,\n",
    "            )\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "    \n",
    "    def get_sample_batch(self):\n",
    "        \"\"\"Get a sample batch for model initialization.\"\"\"\n",
    "        self.setup()\n",
    "        return next(iter(self.train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    \"\"\"Normalize targets to zero mean and unit variance.\"\"\"\n",
    "    \n",
    "    def __init__(self, tensor=None):\n",
    "        if tensor is not None:\n",
    "            self.mean = tensor.mean()\n",
    "            self.std = tensor.std()\n",
    "        else:\n",
    "            self.mean = 0.0\n",
    "            self.std = 1.0\n",
    "            \n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "    \n",
    "    def denorm(self, tensor):\n",
    "        return tensor * self.std + self.mean\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean, 'std': self.std}\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lightning Module - Energy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGFormerModule(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning module for CGFormer energy prediction.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        orig_atom_fea_len: int,\n",
    "        nbr_fea_len: int,\n",
    "        atom_fea_len: int = 64,\n",
    "        n_conv: int = 3,\n",
    "        h_fea_len: int = 128,\n",
    "        n_h: int = 1,\n",
    "        graphormer_layers: int = 1,\n",
    "        num_heads: int = 4,\n",
    "        learning_rate: float = 1e-3,\n",
    "        weight_decay: float = 1e-4,\n",
    "        normalizer: Optional[Normalizer] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['normalizer'])\n",
    "        \n",
    "        self.model = CrystalGraphConvNet(\n",
    "            orig_atom_fea_len=orig_atom_fea_len,\n",
    "            nbr_fea_len=nbr_fea_len,\n",
    "            atom_fea_len=atom_fea_len,\n",
    "            n_conv=n_conv,\n",
    "            h_fea_len=h_fea_len,\n",
    "            n_h=n_h,\n",
    "            graphormer_layers=graphormer_layers,\n",
    "            num_heads=num_heads,\n",
    "            classification=False,\n",
    "        )\n",
    "        \n",
    "        self.normalizer = normalizer or Normalizer()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n",
    "        return self.model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "    \n",
    "    def _shared_step(self, batch, batch_idx):\n",
    "        (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx), target, _ = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "        \n",
    "        # Normalize target\n",
    "        target_normed = self.normalizer.norm(target)\n",
    "        \n",
    "        # Loss\n",
    "        loss = self.criterion(output, target_normed)\n",
    "        \n",
    "        # MAE (denormalized)\n",
    "        pred_denorm = self.normalizer.denorm(output)\n",
    "        mae = F.l1_loss(pred_denorm, target)\n",
    "        \n",
    "        return loss, mae\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, mae = self._shared_step(batch, batch_idx)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_mae', mae, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, mae = self._shared_step(batch, batch_idx)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_mae', mae, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, mae = self._shared_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_mae', mae)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_mae'\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lightning Module - Swap + REINFORCE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swap_utils import (\n",
    "    parse_poscar_string, poscar_to_tensors,\n",
    "    sample_sublattice_swap, apply_n_swaps,\n",
    "    log_prob_sublattice_swap\n",
    ")\n",
    "\n",
    "\n",
    "class SwapScoreNet(nn.Module):\n",
    "    \"\"\"Network that outputs swap scores per atom.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, N, input_dim] -> [batch, N]\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class SwapREINFORCEModule(pl.LightningModule):\n",
    "    \"\"\"REINFORCE training for swap-based structure optimization.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        energy_model: nn.Module,\n",
    "        input_dim: int = 5,  # one-hot atom type\n",
    "        hidden_dim: int = 128,\n",
    "        learning_rate: float = 1e-4,\n",
    "        n_swaps_per_step: int = 10,\n",
    "        reinforce_samples: int = 8,\n",
    "        entropy_reg: float = 0.01,\n",
    "        baseline_ema: float = 0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['energy_model'])\n",
    "        \n",
    "        self.energy_model = energy_model\n",
    "        self.energy_model.eval()\n",
    "        for p in self.energy_model.parameters():\n",
    "            p.requires_grad = False\n",
    "            \n",
    "        self.score_net = SwapScoreNet(input_dim, hidden_dim)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_swaps = n_swaps_per_step\n",
    "        self.reinforce_samples = reinforce_samples\n",
    "        self.entropy_reg = entropy_reg\n",
    "        self.baseline_ema = baseline_ema\n",
    "        \n",
    "        self.register_buffer('baseline', torch.tensor(0.0))\n",
    "        \n",
    "    def forward(self, atom_types_onehot):\n",
    "        \"\"\"Get swap scores for each atom.\"\"\"\n",
    "        return self.score_net(atom_types_onehot)\n",
    "    \n",
    "    def compute_energy(self, atom_types, tensors):\n",
    "        \"\"\"Compute energy using frozen energy model.\n",
    "        \n",
    "        Note: This is a placeholder. Real implementation needs\n",
    "        to convert atom_types to proper crystal graph features.\n",
    "        \"\"\"\n",
    "        # Placeholder: use negative sum as \"energy\"\n",
    "        return -atom_types.float().sum(dim=-1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        atom_types, tensors = batch\n",
    "        batch_size, N = atom_types.shape\n",
    "        device = atom_types.device\n",
    "        \n",
    "        # One-hot encoding\n",
    "        n_types = 5  # Sr, Ti, Fe, O, VO\n",
    "        atom_types_onehot = F.one_hot(atom_types, n_types).float()\n",
    "        \n",
    "        # Get swap scores\n",
    "        scores = self(atom_types_onehot)  # [batch, N]\n",
    "        \n",
    "        # Sample swaps and compute REINFORCE loss\n",
    "        b_site_mask = tensors['b_site_mask']\n",
    "        type_map = tensors['type_map']\n",
    "        ti, fe = type_map['Ti'], type_map['Fe']\n",
    "        \n",
    "        total_log_prob = 0.0\n",
    "        total_energy = 0.0\n",
    "        current = atom_types.clone()\n",
    "        \n",
    "        for _ in range(self.n_swaps):\n",
    "            # Sample swap\n",
    "            swapped, indices = sample_sublattice_swap(\n",
    "                current, b_site_mask, ti, fe, scores\n",
    "            )\n",
    "            \n",
    "            # Log probability\n",
    "            log_prob = log_prob_sublattice_swap(\n",
    "                scores, b_site_mask, ti, fe, current, indices\n",
    "            )\n",
    "            \n",
    "            total_log_prob = total_log_prob + log_prob\n",
    "            current = swapped\n",
    "        \n",
    "        # Compute final energy\n",
    "        energy = self.compute_energy(current, tensors)\n",
    "        \n",
    "        # REINFORCE loss with baseline\n",
    "        advantage = energy - self.baseline\n",
    "        reinforce_loss = (advantage.detach() * total_log_prob).mean()\n",
    "        \n",
    "        # Entropy regularization\n",
    "        entropy_loss = -self.entropy_reg * total_log_prob.mean()\n",
    "        \n",
    "        loss = reinforce_loss + entropy_loss\n",
    "        \n",
    "        # Update baseline\n",
    "        self.baseline = self.baseline_ema * self.baseline + (1 - self.baseline_ema) * energy.mean().detach()\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('energy', energy.mean(), prog_bar=True)\n",
    "        self.log('baseline', self.baseline)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.score_net.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training - Energy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = './STFO_data'  # Change to your data path\n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data exists\n",
    "import os\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"Data directory found: {DATA_DIR}\")\n",
    "    print(f\"Files: {os.listdir(DATA_DIR)[:10]}...\")\n",
    "else:\n",
    "    print(f\"Data directory not found: {DATA_DIR}\")\n",
    "    print(\"Please set DATA_DIR to your crystal data path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "data_module = CrystalDataModule(\n",
    "    root_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    ")\n",
    "\n",
    "# Setup and get sample\n",
    "data_module.setup()\n",
    "sample_batch = data_module.get_sample_batch()\n",
    "(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx), target, cif_ids = sample_batch\n",
    "\n",
    "print(f\"Atom features shape: {atom_fea.shape}\")\n",
    "print(f\"Neighbor features shape: {nbr_fea.shape}\")\n",
    "print(f\"Target shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect targets for normalization\n",
    "train_targets = []\n",
    "for batch in data_module.train_dataloader():\n",
    "    _, target, _ = batch\n",
    "    train_targets.append(target)\n",
    "train_targets = torch.cat(train_targets, dim=0)\n",
    "\n",
    "normalizer = Normalizer(train_targets)\n",
    "print(f\"Target mean: {normalizer.mean:.4f}\")\n",
    "print(f\"Target std: {normalizer.std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = CGFormerModule(\n",
    "    orig_atom_fea_len=atom_fea.shape[-1],\n",
    "    nbr_fea_len=nbr_fea.shape[-1],\n",
    "    atom_fea_len=64,\n",
    "    n_conv=3,\n",
    "    h_fea_len=128,\n",
    "    n_h=1,\n",
    "    graphormer_layers=1,\n",
    "    num_heads=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    normalizer=normalizer,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_mae',\n",
    "    dirpath='./checkpoints',\n",
    "    filename='cgformer-{epoch:02d}-{val_mae:.4f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_mae',\n",
    "    patience=20,\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training - Swap + REINFORCE (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSCAR example for swap training\n",
    "poscar_str = \"\"\"SrTiFeO\n",
    "1.000000\n",
    "11.199000 0.000000 0.000000\n",
    "0.000000 11.199000 0.000000\n",
    "0.000000 0.000000 15.983000\n",
    "Sr Ti Fe O VO\n",
    "32 16 16 88 8\n",
    "Direct\n",
    "0.000000 0.250000 0.125000\n",
    "0.000000 0.250000 0.625000\n",
    "0.000000 0.750000 0.125000\n",
    "0.000000 0.750000 0.625000\n",
    "0.500000 0.250000 0.125000\n",
    "0.500000 0.250000 0.625000\n",
    "0.500000 0.750000 0.125000\n",
    "0.500000 0.750000 0.625000\n",
    "0.000000 0.250000 0.375000\n",
    "0.000000 0.250000 0.875000\n",
    "0.000000 0.750000 0.375000\n",
    "0.000000 0.750000 0.875000\n",
    "0.500000 0.250000 0.375000\n",
    "0.500000 0.250000 0.875000\n",
    "0.500000 0.750000 0.375000\n",
    "0.500000 0.750000 0.875000\n",
    "0.250000 0.000000 0.125000\n",
    "0.250000 0.000000 0.625000\n",
    "0.250000 0.500000 0.125000\n",
    "0.250000 0.500000 0.625000\n",
    "0.750000 0.000000 0.125000\n",
    "0.750000 0.000000 0.625000\n",
    "0.750000 0.500000 0.125000\n",
    "0.750000 0.500000 0.625000\n",
    "0.250000 0.000000 0.375000\n",
    "0.250000 0.000000 0.875000\n",
    "0.250000 0.500000 0.375000\n",
    "0.250000 0.500000 0.875000\n",
    "0.750000 0.000000 0.375000\n",
    "0.750000 0.000000 0.875000\n",
    "0.750000 0.500000 0.375000\n",
    "0.750000 0.500000 0.875000\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo swap operations\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "poscar = parse_poscar_string(poscar_str)\n",
    "tensors = poscar_to_tensors(poscar, device=device)\n",
    "\n",
    "# Create batch\n",
    "batch_size = 128\n",
    "atom_types = tensors['atom_types'].unsqueeze(0).expand(batch_size, -1).clone()\n",
    "\n",
    "print(f\"Atom types shape: {atom_types.shape}\")\n",
    "print(f\"B-site count: {tensors['b_site_mask'].sum().item()}\")\n",
    "print(f\"O-site count: {tensors['o_site_mask'].sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test swap\n",
    "import time\n",
    "\n",
    "n_swaps = 100\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "start = time.time()\n",
    "\n",
    "swapped, history = apply_n_swaps(\n",
    "    atom_types,\n",
    "    tensors['b_site_mask'],\n",
    "    tensors['o_site_mask'],\n",
    "    tensors['type_map'],\n",
    "    n_swaps=n_swaps,\n",
    "    swap_mode='both'\n",
    ")\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"{batch_size} samples Ã— {n_swaps} swaps = {batch_size * n_swaps:,} total\")\n",
    "print(f\"Time: {elapsed:.3f}s\")\n",
    "print(f\"Throughput: {(batch_size * n_swaps) / elapsed:,.0f} swaps/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save/Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# trainer.save_checkpoint('cgformer_final.ckpt')\n",
    "\n",
    "# Load\n",
    "# model = CGFormerModule.load_from_checkpoint('cgformer_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **CGFormerModule** - Lightning module for energy prediction\n",
    "   - MSE loss with target normalization\n",
    "   - MAE metric tracking\n",
    "   - ReduceLROnPlateau scheduler\n",
    "\n",
    "2. **SwapREINFORCEModule** - Lightning module for swap policy learning\n",
    "   - REINFORCE with baseline\n",
    "   - Entropy regularization\n",
    "   - Frozen energy model\n",
    "\n",
    "3. **CrystalDataModule** - Data loading wrapper\n",
    "   - Train/val/test split\n",
    "   - Collate function for crystal graphs\n",
    "\n",
    "4. **Swap utilities** - GPU-accelerated swap operations\n",
    "   - Gumbel-max sampling\n",
    "   - Beam search\n",
    "   - Log probability computation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
